{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f02fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fdcb5",
   "metadata": {},
   "source": [
    "* Running Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1613ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run partA_utilities.ipynbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f6101",
   "metadata": {},
   "source": [
    "* Defining Mode and creating directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4a48ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory for mode: prd already exists\n"
     ]
    }
   ],
   "source": [
    "mode = \"prd\"\n",
    "\n",
    "make_mode_file(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4331c",
   "metadata": {},
   "source": [
    "* Defining Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b99b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(mode = mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4358f49",
   "metadata": {},
   "source": [
    "* Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28efc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(settings.sources_sinks_path + '/test_data_processed.csv', index_col=0)\n",
    "X_test = test_data.loc[:, [\"title\", \"content\", \"site_title\", \"theme\"]]\n",
    "y_test = test_data.loc[:, [\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeeae1b",
   "metadata": {},
   "source": [
    "* Transforming List of String Columns back to Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad957ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transform_dataframe_list_columns_to_arrays(X_test, \n",
    "                                                   list_of_string_cols = [\"title\", \n",
    "                                                                          \"content\", \n",
    "                                                                          \"site_title\", \n",
    "                                                                          \"theme\"]\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8874491",
   "metadata": {},
   "source": [
    "* Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "541ad0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(settings.sources_sinks_path + '/dl_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f7ae6",
   "metadata": {},
   "source": [
    "* Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fdd2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict([get_array_from_pandas_col_of_arrays(X_test, \"title\"),\n",
    "                                  get_array_from_pandas_col_of_arrays(X_test, \"content\"),\n",
    "                                  get_array_from_pandas_col_of_arrays(X_test, \"site_title\"),\n",
    "                                  get_array_from_pandas_col_of_arrays(X_test, \"theme\"),\n",
    "                                  ])\n",
    "\n",
    "test_predictions = [np.argmax(x) for x in test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167d06c",
   "metadata": {},
   "source": [
    "* Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522e8d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score and Classification Report on unseen data\n",
      "0.7391304347826086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90        36\n",
      "           1       0.89      0.60      0.71        52\n",
      "           2       0.96      0.50      0.66        48\n",
      "           3       0.75      0.78      0.76        27\n",
      "           4       0.66      1.00      0.79        27\n",
      "           5       0.34      0.96      0.50        27\n",
      "           6       1.00      1.00      1.00        27\n",
      "           7       0.79      0.70      0.75        27\n",
      "           8       0.80      0.82      0.81        40\n",
      "           9       0.52      0.89      0.66        27\n",
      "          10       0.88      0.81      0.84        47\n",
      "          11       1.00      0.68      0.81        40\n",
      "          12       0.93      0.74      0.83        35\n",
      "          13       0.93      0.84      0.88        31\n",
      "          14       0.86      0.70      0.78        27\n",
      "          15       0.33      0.74      0.45        27\n",
      "          16       0.93      0.96      0.95        27\n",
      "          17       0.90      0.70      0.79        27\n",
      "          18       0.83      0.89      0.86        27\n",
      "          19       0.71      0.77      0.74        31\n",
      "          20       0.94      0.29      0.44        56\n",
      "\n",
      "    accuracy                           0.74       713\n",
      "   macro avg       0.81      0.77      0.76       713\n",
      "weighted avg       0.83      0.74      0.75       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score and Classification Report on unseen data\")\n",
    "print(accuracy_score(y_test,test_predictions))\n",
    "print(classification_report(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd1d9a",
   "metadata": {},
   "source": [
    "* We can see that we manage an accruacy of 0.74. In fact, the analysis indicates that in several classes, the model is pretty strong (e.g. Religion&Spirituality --> 13). \n",
    "\n",
    "\n",
    "\n",
    "* As long as other classes are concerned, the model seems to underperform (e.g. Viral Articles). All in all, the results are encouraging and our modeling approach seems to perform pretty well.\n",
    "\n",
    "\n",
    "* An interesting point is that, for the case of Sensitive Topics, we achieve a recall of 0.78, which is also encouraging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136939fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        original_label_col  label\n",
      "522              Astrology      0\n",
      "600            Attractions      1\n",
      "259             Automotive      2\n",
      "560                 Beauty      3\n",
      "377       Business&Finance      4\n",
      "363                Culture      5\n",
      "268              Education      6\n",
      "538   Family&Relationships      7\n",
      "38              Food&Drink      8\n",
      "584         Healthy Living      9\n",
      "88             Home&Garden     10\n",
      "579               Politics     11\n",
      "19             Pop Culture     12\n",
      "503  Religion&Spirituality     13\n",
      "517                Science     14\n",
      "558       Sensitive Topics     15\n",
      "95                  Sports     16\n",
      "592          Style&Fashion     17\n",
      "244         Tech&Computing     18\n",
      "643                 Travel     19\n",
      "573         Viral Articles     20\n"
     ]
    }
   ],
   "source": [
    "label_mapping_dataframe = pd.read_csv(settings.sources_sinks_path + '/label_mapping_dataframe.csv',\n",
    "                                     index_col=0)\n",
    "\n",
    "print(label_mapping_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

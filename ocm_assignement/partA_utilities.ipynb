{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fae591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import ast\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Flatten, concatenate\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "#################################### Utility Functions ####################################\n",
    "def secure_valid_modes(mode):\n",
    "    \"\"\" Function for securing that the mode is one of the [dev, stg, prd]\n",
    "        This tries to mimic the format of a working code base\n",
    "    \"\"\"\n",
    "    if mode not in [\"dev\", \"stg\", \"prd\"]:\n",
    "        raise Exception(\n",
    "        \"\"\"The provided mode is not in [dev, stg, prd]\n",
    "           Please Provide one of the above modes\n",
    "        \"\"\"\n",
    "        )\n",
    "        \n",
    "def make_mode_file(mode):\n",
    "    \"\"\" \n",
    "    Function for creating the appropriate mode directory    \n",
    "    \"\"\"\n",
    "    outdir = mode\n",
    "    secure_valid_modes(mode)\n",
    "    if not os.path.exists(outdir):\n",
    "        print(f\"Making directory for mode: {mode}\")\n",
    "        os.mkdir(outdir)\n",
    "    else:\n",
    "        print(f\"Directory for mode: {mode} already exists\")\n",
    "\n",
    "def stopword_and_punct_removal(text, nlp):\n",
    "    \"\"\" \n",
    "    Function for removing stopwords and punctuation from a given text\n",
    "    \"\"\"\n",
    "    return \" \".join([token.text for token in nlp(text) \n",
    "                     if not (token.is_stop | token.is_punct)]\n",
    "                   )\n",
    "\n",
    "def lemmatization(text, nlp):\n",
    "    \"\"\" \n",
    "    Function applying lemmatization on text\n",
    "    \"\"\"\n",
    "    return ' '.join([word.lemma_ for word in nlp(text)])\n",
    "\n",
    "def get_vector_embeddings(text, nlp):\n",
    "    \"\"\" \n",
    "    Function to apply vector embeddings\n",
    "    \"\"\"\n",
    "    return nlp(text).vector.tolist()\n",
    "\n",
    "def string_list_to_array(list_obj):\n",
    "    \"Converts a string list to array\"\n",
    "    return np.array(ast.literal_eval(list_obj))\n",
    "\n",
    "def transform_dataframe_list_columns_to_arrays(dataframe,\n",
    "                                               list_of_string_cols\n",
    "                                              ):\n",
    "    \"\"\"\n",
    "    # TODO: Remove this functionality and go directly to the function below.\n",
    "    Transforms all of the provided columns of a dataframe to array-columns.\n",
    "    Keep in mind that, after the application of this, the shape of a single row\n",
    "    is of dimensionality of 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in list_of_string_cols:\n",
    "        dataframe[column] = dataframe[column].apply(lambda x: string_list_to_array(x))\n",
    "        \n",
    "    return dataframe\n",
    "\n",
    "def get_array_from_pandas_col_of_arrays(dataframe,\n",
    "                                        column_name\n",
    "                                        ):\n",
    "    \"\"\"\n",
    "    Transforms a pandas dataframe column of arrays to a single array,\n",
    "    with dimensionality of (len(dataframe), len(array))\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.array(dataframe[column_name].to_list())\n",
    "    \n",
    "    return array\n",
    "#################################### Main Objects ############################################\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"\n",
    "    Includes the main settings for the project\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        self.sources_sinks_path = './sources_sinks'\n",
    "        self.train_data_path = self.sources_sinks_path + '/train_data.csv'\n",
    "        self.test_data_path = self.sources_sinks_path + '/unseen_test_data.csv'\n",
    "\n",
    "        \n",
    "class DataPreprocessing:\n",
    "    \"\"\"\n",
    "    Defines the main functionality for preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode, settings, training_mode = True):\n",
    "        self.mode = mode\n",
    "        self.settings = Settings(mode=mode)\n",
    "        self.greek_nlp = spacy.load(\"el_core_news_lg\")\n",
    "        self.eng_nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.greek_columns_list = ['title' , 'content']\n",
    "        self.english_columns_list = ['theme' , 'site_title']\n",
    "        self.label_column = 'label'\n",
    "        self.columns_to_apply_stopword_removal = ['title' , 'content']\n",
    "        self.columns_to_apply_lemmatization = ['title' , 'content', 'theme' , 'site_title']\n",
    "        self.columns_to_vectorize = ['title' , 'content', 'theme' , 'site_title']\n",
    "        self.training_mode = training_mode\n",
    "        self.label_mapping_dataframe = None\n",
    "        \n",
    "    def _get_appropriate_nlp(self, column):\n",
    "        if column in self.greek_columns_list:\n",
    "                    nlp = self.greek_nlp\n",
    "        else:\n",
    "            nlp = self.eng_nlp\n",
    "            \n",
    "        return nlp\n",
    "    \n",
    "    def create_site_titles_and_themes(self, dataset):\n",
    "        list_of_site_titles = []\n",
    "        list_of_themes = []\n",
    "        \n",
    "        for index, row in dataset.iterrows():\n",
    "            url_str = row['url']\n",
    "            url_suffix = re.search('\\.gr|\\.com|\\.net', url_str).group(0)\n",
    "            split_sentece = url_str.split(url_suffix)\n",
    "            site_title = split_sentece[0].replace('https://', \"\").replace('http://', \"\").replace('www.', \"\")\n",
    "            theme = split_sentece[1].split('/')[1]\n",
    "            list_of_site_titles.append(site_title)\n",
    "            list_of_themes.append(theme)\n",
    "            \n",
    "        dataset['site_title'] = list_of_site_titles\n",
    "        dataset['theme'] = list_of_themes\n",
    "        dataset.drop('url', axis = 1, inplace = True)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def remove_stopwords_and_punctuation(self, dataset):\n",
    "        for column in self.columns_to_apply_stopword_removal:\n",
    "            nlp = self._get_appropriate_nlp(column)  \n",
    "            dataset[column] = dataset[column].apply(lambda text:\n",
    "                                                    stopword_and_punct_removal(text, nlp)\n",
    "                                                    )\n",
    "        return dataset\n",
    "    \n",
    "        \n",
    "    def lemmatization(self, dataset):\n",
    "        for column in self.columns_to_apply_lemmatization:\n",
    "            nlp = self._get_appropriate_nlp(column)                         \n",
    "            dataset[column] = dataset[column].apply(lambda text:\n",
    "                                                    lemmatization(text, nlp)\n",
    "                                                    )\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def construct_embeddings(self, dataset):\n",
    "        for column in self.columns_to_vectorize:\n",
    "            nlp = self._get_appropriate_nlp(column)\n",
    "            dataset[column] = dataset[column].apply(lambda text:\n",
    "                                                    get_vector_embeddings(text, nlp)\n",
    "                                                    )\n",
    "        return dataset\n",
    "    \n",
    "    def encode_target_labels(self, dataset):\n",
    "        dataset['original_label_col'] = dataset[self.label_column]\n",
    "        if self.training_mode:\n",
    "            le = LabelEncoder()\n",
    "            dataset[self.label_column] = le.fit(dataset[self.label_column]).transform(dataset[self.label_column])\n",
    "            joblib.dump(le, self.settings.sources_sinks_path + '/label_encoder',compress=9)\n",
    "        else:\n",
    "            le=joblib.load(self.settings.sources_sinks_path + '/label_encoder')\n",
    "            dataset[self.label_column] = le.transform(self.label_column)\n",
    "            \n",
    "        self.label_mapping_dataframe = (dataset\n",
    "                                        .sort_values(by = [self.label_column])\n",
    "                                        .drop_duplicates([self.label_column])\n",
    "                                        .loc[:, [\"original_label_col\" , self.label_column]]\n",
    "                                        )\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def write_mapping_dataframe_to_sinks(self):\n",
    "        self.label_mapping_dataframe.to_csv(settings.sources_sinks_path + \n",
    "                                            '/label_mapping_dataframe.csv'\n",
    "                                           )\n",
    "    \n",
    "    def write_to_sinks(self, dataset, write_path):\n",
    "        dataset.to_csv(write_path)\n",
    "        \n",
    "class DummyBaseline:\n",
    "    \"\"\"\n",
    "    Defines the main functionality of a baseline dummy classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 mode, \n",
    "                 X_train,\n",
    "                 X_val,\n",
    "                 y_train,\n",
    "                 y_val,\n",
    "                 settings):\n",
    "        \n",
    "        self.predictions = None\n",
    "        self.mode = mode\n",
    "        self.settings = settings\n",
    "        self.classifier = DummyClassifier(strategy=\"uniform\")\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        \n",
    "    def model_fit(self):\n",
    "            self.classifier.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def model_predict(self):\n",
    "        self.predictions = self.classifier.predict(self.X_val)\n",
    "        \n",
    "    \n",
    "    def write_predictions(self):\n",
    "        pd.DataFrame(self.predictions, \n",
    "                     columns = ['predictions']).to_csv(self.settings.sources_sinks_path +\n",
    "                                '/dummy_classifier_val_predictions.csv'\n",
    "                               )\n",
    "    \n",
    "    def execute_pipeline(self):\n",
    "        self.model_fit()\n",
    "        self.model_predict()\n",
    "        self.write_predictions()\n",
    "    \n",
    "class SingleFeatureDecisionTreeBaseline:\n",
    "    \"\"\" \n",
    "    Defines the main functionality of a baseline decision tree classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 mode, \n",
    "                 X_train,\n",
    "                 X_val,\n",
    "                 y_train,\n",
    "                 y_val,\n",
    "                 settings\n",
    "                ):\n",
    "        self.predictions = None\n",
    "        self.mode = mode\n",
    "        self.settings = settings\n",
    "        self.classifier = DecisionTreeClassifier()\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    \n",
    "    def model_fit(self):\n",
    "        self.classifier.fit(self.X_train, self.y_train)\n",
    "\n",
    "        \n",
    "    def model_predict(self):\n",
    "        self.predictions = self.classifier.predict(self.X_val)\n",
    "        \n",
    "        \n",
    "    def write_predictions(self):\n",
    "        pd.DataFrame(self.predictions, \n",
    "                     columns = ['predictions']).to_csv(self.settings.sources_sinks_path +\n",
    "                                '/decision_tree_val_predictions.csv'\n",
    "                                )\n",
    "        \n",
    "    def execute_pipeline(self):\n",
    "        self.model_fit()\n",
    "        self.model_predict()\n",
    "        self.write_predictions()\n",
    "        \n",
    "\n",
    "class MultiInputDenseDL:\n",
    "    \"\"\" \n",
    "    Defines the main functionality of multi input Dense Deep Learning Classifier.\n",
    "    Each of the 4 features serves as a single input, and all of them are concatenated to a\n",
    "    final layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 mode, \n",
    "                 X_train,\n",
    "                 X_val,\n",
    "                 y_train,\n",
    "                 y_val,\n",
    "                 settings\n",
    "                ):\n",
    "        self.predictions = None\n",
    "        self.mode = mode\n",
    "        self.settings = settings\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.num_labels = len(np.unique(y_train))\n",
    "        self.dense_units = 32\n",
    "        self.rate = 0.5\n",
    "        self.model = None\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 128\n",
    "        self.callback = EarlyStopping(monitor='val_accuracy',\n",
    "                                      patience=20)\n",
    "    \n",
    "    \n",
    "    def preprocess_labels(self):\n",
    "        self.y_train = to_categorical(y_train)\n",
    "        self.y_val = to_categorical(y_val)\n",
    "    \n",
    "    def model_construct(self):\n",
    "        ### Constructing Title\n",
    "        input_title = Input(shape = (300,1))\n",
    "        title = Dense(units=self.dense_units, activation=\"relu\")(input_title)\n",
    "        title = Dropout(rate = self.rate)(title)\n",
    "        ### Constructing content\n",
    "        input_content = Input(shape = (300,1))\n",
    "        content = Dense(units = self.dense_units, activation=\"relu\")(input_content)\n",
    "        content = Dropout(rate = self.rate)(content)\n",
    "        ### Constructing site_title\n",
    "        input_site_title = Input(shape = (300,1))\n",
    "        site_title = Dense(units = self.dense_units, activation=\"relu\")(input_site_title)\n",
    "        site_title = Dropout(rate = self.rate)(site_title)\n",
    "        ### Constructing theme\n",
    "        input_theme = Input(shape = (300,1))\n",
    "        theme = Dense(units = self.dense_units, activation=\"relu\")(input_theme)\n",
    "        theme = Dropout(rate = self.rate)(theme)\n",
    "        # Concatenating layers\n",
    "        merge=concatenate([title, content, site_title, theme])\n",
    "        # feature maps to vector before connecting to Dense\n",
    "        flatten = Flatten()(merge)\n",
    "        outputs = Dense(self.num_labels, activation='softmax')(flatten)\n",
    "        self.model = Model([input_title, input_content, input_site_title, input_theme], outputs)\n",
    "    \n",
    "    def model_compile(self):\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'],\n",
    "                          )\n",
    "    \n",
    "    def model_fit(self):\n",
    "        self.model.fit([get_array_from_pandas_col_of_arrays(X_train, \"title\"),\n",
    "                        get_array_from_pandas_col_of_arrays(X_train, \"content\"),\n",
    "                        get_array_from_pandas_col_of_arrays(X_train, \"site_title\"),\n",
    "                        get_array_from_pandas_col_of_arrays(X_train, \"theme\"),\n",
    "                        ],\n",
    "                        self.y_train,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=self.epochs,\n",
    "                        batch_size = self.batch_size,\n",
    "                        callbacks = [self.callback],\n",
    "                        verbose=1\n",
    "                       )\n",
    "    \n",
    "    def model_predict(self):\n",
    "        self.predictions = self.model.predict([get_array_from_pandas_col_of_arrays(X_val, \"title\"),\n",
    "                                               get_array_from_pandas_col_of_arrays(X_val, \"content\"),\n",
    "                                               get_array_from_pandas_col_of_arrays(X_val, \"site_title\"),\n",
    "                                               get_array_from_pandas_col_of_arrays(X_val, \"theme\"),\n",
    "                                               ]\n",
    "                                              )\n",
    "        \n",
    "        self.predictions = [np.argmax(x) for x in self.predictions]\n",
    "    \n",
    "    def write_predictions(self):\n",
    "        pd.DataFrame(self.predictions, \n",
    "                     columns = ['predictions']).to_csv(self.settings.sources_sinks_path +\n",
    "                                '/multi_dense_val_predictions.csv'\n",
    "                                )\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save(self.settings.sources_sinks_path + '/dl_model.keras')\n",
    "    \n",
    "    def execute_pipeline(self):\n",
    "        self.preprocess_labels()\n",
    "        self.model_construct()\n",
    "        self.model_compile()\n",
    "        self.model_fit()\n",
    "        self.model_predict()\n",
    "        self.write_predictions()\n",
    "        self.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
